{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ModelMetaclass' from 'pydantic.main' (C:\\Users\\Bruger\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pydantic\\main.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed, shuffle, sample\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\errors.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\compat.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\config.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\confection\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, create_model, ValidationError, Extra\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ModelMetaclass' from 'pydantic.main' (C:\\Users\\Bruger\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pydantic\\main.py)"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "from random import seed, shuffle, sample\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation of ingredients \n",
    "\n",
    "This notebook includes the annotation and prediction of ingredients.\n",
    "\n",
    "The process is structured in the following way:\n",
    "\n",
    "- Extracting a sample of the raw ingredient data from the dataset, making some preliminary predictions, and saving it as _.jsonl_ for use in the annotation software _Doccano_. The preliminary annotations are made with another model, that we did not end up using, and serve to ease the annotation process.\n",
    "- (not in the notebook) Annotating the data using _Doccano_\n",
    "- Constructing a training dataset from our annotations, using a majority vote system (here we only use 3 annotators).\n",
    "- Training a model with _spacy_, using on these training data.\n",
    "- Computing labels for the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing preliminary annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = '../../../data/raw/data_raw.json'\n",
    "\n",
    "LABELING_MODEL_PATH = \"PREVIOUS MODEL INSERT PATH HERE (not needed anymore)\"\n",
    "\n",
    "ANNOTATION_FOLDER = '../../../data/interim/annotation/'\n",
    "TO_ANNOTATE = ANNOTATION_FOLDER + \"TO_ANNOTATE.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RAW_DATA_PATH, 'r', encoding = 'utf8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# we sample 30 recipes\n",
    "seed(69)\n",
    "keys = sample(list(data.keys()), 30)\n",
    "\n",
    "ingredients = []\n",
    "\n",
    "for key in keys:\n",
    "    ingredients.extend(data[key]['ingredients'])\n",
    "\n",
    "# we trained this model with smaller amount of data\n",
    "nlp = spacy.load(LABELING_MODEL_PATH)\n",
    "\n",
    "outs = []\n",
    "\n",
    "for ingredient in ingredients:\n",
    "\n",
    "    doc = nlp(ingredient)\n",
    "\n",
    "    label = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        label.append([ent.start_char, ent.end_char, 'ingredient'])\n",
    "\n",
    "    outs.append({\"text\": ingredient, \n",
    "                 \"label\": label})\n",
    "    \n",
    "with open(ANNOTATION_FOLDER + TO_ANNOTATE, 'w', encoding=\"utf8\") as file:\n",
    "    for out in outs:\n",
    "        file.write(json.dumps(out, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation\n",
    "\n",
    "_Done in Doccano_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate combined training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_ANNOTATORS = 3\n",
    "COMBINED_PATH = ANNOTATION_FOLDER + 'annotations/COMBINED.jsonl'\n",
    "\n",
    "ANNOTATED_FILES = [\n",
    "    'bogdan.jsonl',\n",
    "    'gino.jsonl',\n",
    "    'veron.jsonl'\n",
    "]\n",
    "\n",
    "TRAIN_PATH = ANNOTATION_FOLDER + \"spacy/train.spacy\"\n",
    "DEV_PATH = ANNOTATION_FOLDER + \"spacy/dev.spacy\"\n",
    "TEST_PATH = ANNOTATION_FOLDER + \"spacy/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_lists = []\n",
    "\n",
    "for annotator_file in ANNOTATED_FILES:\n",
    "    with open(ANNOTATION_FOLDER + 'annotations/' + annotator_file, 'r', encoding='utf-8') as file:\n",
    "        annotator_list = [\n",
    "            json.loads(line)\n",
    "            for line in file.readlines()\n",
    "        ]\n",
    "        annotator_lists.append(sorted(annotator_list, key=lambda x: x['id']))\n",
    "\n",
    "# we only use 3 of the annotators, \n",
    "# because we had issues with majority voting with an even number of annotators\n",
    "\n",
    "final_annotations = []\n",
    "\n",
    "for annotations in zip(*annotator_lists[:3]):\n",
    "    \n",
    "    labels = [\n",
    "        (label[0], label[1])\n",
    "        for annotation in annotations\n",
    "            for label in annotation['label']\n",
    "    ]\n",
    "\n",
    "    to_keep = [\n",
    "        label\n",
    "        for label, count\n",
    "            in Counter(labels).items()\n",
    "        if count / NO_ANNOTATORS > 0.5\n",
    "    ]\n",
    "\n",
    "    final_annotations.append(\n",
    "        {\n",
    "            'text': annotations[0]['text'],\n",
    "            'label': [\n",
    "                [label[0], label[1], 'ingredient']\n",
    "                for label in to_keep\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "with open(COMBINED_PATH, 'w', encoding='utf-8') as file:\n",
    "    for annotation in final_annotations:\n",
    "        file.write(json.dumps(annotation, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(outfile_name, annotations):\n",
    "    '''\n",
    "    takes jsonl-data (annotations) as saves it as a spacy binary dataset\n",
    "    '''\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    db = DocBin()\n",
    "\n",
    "    for annotation in annotations:\n",
    "        doc = nlp(annotation[\"text\"])\n",
    "        ents = []\n",
    "        for start, end, label in annotation['label']:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span != None:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "\n",
    "    db.to_disk(outfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(COMBINED_PATH, 'r', encoding='utf-8') as f:\n",
    "    final_annotations = [\n",
    "        json.loads(line)\n",
    "        for line \n",
    "            in f.readlines()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(100)\n",
    "shuffle(final_annotations)\n",
    "\n",
    "train_i = len(final_annotations) - (len(final_annotations) // 3)\n",
    "dev_i = len(final_annotations) - (len(final_annotations) // 6)\n",
    "\n",
    "train = final_annotations[:train_i]\n",
    "dev = final_annotations[train_i:dev_i]\n",
    "test = final_annotations[dev_i:]\n",
    "\n",
    "make_data(TRAIN_PATH, train)\n",
    "make_data(DEV_PATH, dev)\n",
    "\n",
    "with open(TEST_PATH, 'w', encoding='utf-8') as f:\n",
    "    for annotation in test:\n",
    "        f.write(json.dumps(annotation, ensure_ascii=False))\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training annotator model\n",
    "\n",
    "_Done with spacy in shell_\n",
    "\n",
    "_python -m spacy train ml_models/annotation/config.cfg --output ml_models/annotation/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making synthetic annotations using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = ANNOTATION_FOLDER + 'data.json'\n",
    "MODEL_PATH = '../../../ml_models/annotation/models/model-best/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RAW_DATA_PATH, 'r', encoding = 'utf8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "keys = data.keys()\n",
    "\n",
    "nlp = spacy.load(MODEL_PATH)\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "for key in keys:\n",
    "    ingredients = data[key]['ingredients']\n",
    "    data[key]['ingredient_annotations'] = []\n",
    "    for ingredient in ingredients:\n",
    "        doc = nlp(ingredient)\n",
    "        for ent in doc.ents:\n",
    "            data[key]['ingredient_annotations'].append(' '.join([lem.lemmatize(token.text) for token in ent]).lower())\n",
    "\n",
    "with open(OUT_PATH, 'wb') as f:\n",
    "    f.write(json.dumps(data, indent = 4, ensure_ascii=False).encode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "We test on the test data using f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(MODEL_PATH)\n",
    "\n",
    "with open(TEST_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_data = [\n",
    "        json.loads(ingr)\n",
    "        for ingr in f.readlines()\n",
    "    ]\n",
    "\n",
    "ground_truth = [\n",
    "    [\n",
    "        ingr['text'][label[0]:label[1]] \n",
    "        for label \n",
    "            in ingr['label']\n",
    "    ]\n",
    "    \n",
    "    for ingr in test_data\n",
    "]\n",
    "\n",
    "pred = [\n",
    "    [\n",
    "        ent.text\n",
    "        for ent\n",
    "            in nlp(ingr['text']).ents\n",
    "    ]\n",
    "    \n",
    "    for ingr in test_data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7642276422764227"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer = MultiLabelBinarizer().fit(pred + ground_truth)\n",
    "f1_score(binarizer.transform(pred), binarizer.transform(ground_truth), average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
